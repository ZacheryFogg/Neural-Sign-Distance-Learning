{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import open3d as o3d\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataclasses import dataclass\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "from pytorch3d.loss import chamfer_distance\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import open3d as o3d\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataclasses import dataclass\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "from pytorch3d.loss import chamfer_distance\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from Helpers.data import PointCloudDataset\n",
    "import Helpers.PointCloudOpen3d as pc\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f'Using: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# point_size = 1024\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# train_dataset = PointCloudDataset(\"..Data/ModelNet40\", point_size, 'train')\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# train_loader = DataLoader(train_dataset, batch_size = 256, shuffle = True)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# print(len(train_dataset))\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m train_datasettt \u001b[38;5;241m=\u001b[39m \u001b[43mPointCloudDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../Data/ModelNet40\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoint_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_datasettt))\n",
      "File \u001b[1;32mc:\\Users\\zfogg\\OneDrive\\git\\Neural-Sign-Distance-Learning\\Helpers\\data.py:17\u001b[0m, in \u001b[0;36mPointCloudDataset.__init__\u001b[1;34m(self, base_dir, point_cloud_size, split, object_classes)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobject_classes \u001b[38;5;241m=\u001b[39m object_classes\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_file_paths()\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoint_clouds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_uniform_point_clouds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zfogg\\OneDrive\\git\\Neural-Sign-Distance-Learning\\Helpers\\data.py:62\u001b[0m, in \u001b[0;36mPointCloudDataset.get_uniform_point_clouds\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m point_clouds_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles: \n\u001b[1;32m---> 62\u001b[0m     mesh \u001b[38;5;241m=\u001b[39m \u001b[43mo3d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_triangle_mesh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \n\u001b[0;32m     64\u001b[0m         sampled_point_cloud \u001b[38;5;241m=\u001b[39m mesh\u001b[38;5;241m.\u001b[39msample_points_uniformly(number_of_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoint_cloud_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# point_size = 1024\n",
    "# train_dataset = PointCloudDataset(\"..Data/ModelNet40\", point_size, 'train')\n",
    "# train_loader = DataLoader(train_dataset, batch_size = 256, shuffle = True)\n",
    "# print(len(train_dataset))\n",
    "\n",
    "train_datasettt = PointCloudDataset(\"../Data/ModelNet40\", point_size, 'train')\n",
    "print(len(train_datasettt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(train_loader))['points'][0]\n",
    "cloud = pc.get_point_cloud(x)\n",
    "pc.visualize_point_cloud(cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat = x\n",
    "\n",
    "conv1 = nn.Conv2d(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1024])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat = x.T\n",
    "conv1 = nn.Conv1d(3, 64, 1) # 64 kernels of size (3,1). Each kernel turns 3 points into 1 point \n",
    "conv2 = nn.Conv1d(64, 128, 1) # 128 kernels of size (64,1). Still operating on a per point level \n",
    "out = conv1(feat)\n",
    "out = conv2(out)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvPointCloudAE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        pass \n",
    "\n",
    "    \n",
    "    def encoder(self,x):\n",
    "        pass \n",
    "\n",
    "\n",
    "    def decoder(self,x):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        latent_rep = self.encoder(x)\n",
    "        reconstructed = self.decoder(x)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointCloudAE(nn.Module):\n",
    "    def __init__(self, point_size, latent_size):\n",
    "        super(PointCloudAE, self).__init__()\n",
    "        \n",
    "        self.latent_size = latent_size\n",
    "        self.point_size = point_size\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv1d(3, 1024, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(1024, 256, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(256, self.latent_size, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.bn3 = nn.BatchNorm1d(self.latent_size)\n",
    "        \n",
    "        self.dec1 = nn.Linear(self.latent_size,1024)\n",
    "        self.dec2 = nn.Linear(1024,2048)\n",
    "        self.dec3 = nn.Linear(2048,self.point_size*3)\n",
    "\n",
    "    def encoder(self, x): \n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.bn3(self.conv3(x))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, self.latent_size)\n",
    "        return x\n",
    "    \n",
    "    def decoder(self, x):\n",
    "        x = F.relu(self.dec1(x))\n",
    "        x = F.relu(self.dec2(x))\n",
    "        x = self.dec3(x)\n",
    "        return x.view(-1, self.point_size, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "net = PointCloudAE(point_size, 768)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "net = net.to(device)\n",
    "\n",
    "from pytorch3d.loss import chamfer_distance # chamfer distance for calculating point cloud distance\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0049487215542602676\n",
      "0.001842792481617179\n",
      "0.0014271337138272303\n",
      "0.001220517770148987\n",
      "0.0010973226582241613\n",
      "0.0010287557866662568\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 28\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50000\u001b[39m) :\n\u001b[0;32m     26\u001b[0m     startTime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 28\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#train one epoch, get the average loss\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     train_loss_list\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m     32\u001b[0m     epoch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m startTime\n",
      "Cell \u001b[1;32mIn[44], line 14\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     12\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 14\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "def train_epoch():\n",
    "    epoch_loss = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        data = data['points'].to(device)\n",
    "        output = net(data.permute(0,2,1)) # transpose data for NumberxChannelxSize format\n",
    "        loss, _ = chamfer_distance(data, output) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    if i == 0:\n",
    "        return epoch_loss / 1\n",
    "    return epoch_loss/i\n",
    "\n",
    "\n",
    "train_loss_list = []  \n",
    "test_loss_list = []  \n",
    "\n",
    "for i in range(50000) :\n",
    "\n",
    "    startTime = time.time()\n",
    "    \n",
    "    train_loss = train_epoch() #train one epoch, get the average loss\n",
    "    train_loss_list.append(train_loss)\n",
    "\n",
    "    \n",
    "    epoch_time = time.time() - startTime\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), './50kepoch_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(train_loader))['points'][0]\n",
    "\n",
    "cloud = pc.get_point_cloud(x)\n",
    "pc.visualize_point_cloud(cloud)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    xx = x.T.unsqueeze(0).to(device)\n",
    "    rec_x = np.array(net(xx)[0].to('cpu'))\n",
    "    cloud = pc.get_point_cloud(rec_x)\n",
    "    pc.visualize_point_cloud(cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4817, 0.6104, 0.3859],\n",
      "        [0.4709, 0.6067, 0.4072],\n",
      "        [0.4908, 0.6125, 0.3900],\n",
      "        ...,\n",
      "        [0.5262, 0.5955, 0.4208],\n",
      "        [0.5483, 0.8067, 0.4412],\n",
      "        [0.5769, 0.8080, 0.4399]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = next(iter(train_loader))['points'][0]\n",
    "\n",
    "# print(x[:,i].min())\n",
    "x_min = x.min()\n",
    "x_max = x.max()\n",
    "\n",
    "x_norm = (x - x_min) / (x_max - x_min)\n",
    "\n",
    "cloud = pc.get_point_cloud(x_norm)\n",
    "pc.visualize_point_cloud(cloud)\n",
    "\n",
    "print(x_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5825, 0.5014, 0.1379],\n",
       "        [0.5915, 0.4976, 0.1370],\n",
       "        [0.8215, 0.3296, 0.1172],\n",
       "        ...,\n",
       "        [0.6037, 0.3774, 0.0789],\n",
       "        [0.6028, 0.3761, 0.0731],\n",
       "        [0.6030, 0.3761, 0.0733]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_norm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-open3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
