{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import open3d as o3d\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataclasses import dataclass\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from pytorch3d.loss import chamfer_distance\n",
    "\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from Helpers.data import PointCloudDataset\n",
    "import Helpers.PointCloudOpen3d as pc\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f'Using: {device}')\n",
    "\n",
    "from Helpers.data2 import PointCloudDataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airplane : 625\n",
      "bed : 514\n",
      "bookshelf : 571\n",
      "bottle : 334\n",
      "car : 196\n",
      "chair : 888\n",
      "desk : 199\n",
      "dresser : 199\n",
      "mantel : 283\n",
      "monitor : 464\n",
      "night : 199\n",
      "piano : 230\n",
      "plant : 239\n",
      "sofa : 679\n",
      "table : 391\n",
      "toilet : 343\n",
      "tv : 266\n",
      "vase : 474\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "names = {}\n",
    "for root, _, files in os.walk(\"../Data/ModelNet40\"):\n",
    "    for file in files:\n",
    "        if file.endswith('.off'):\n",
    "            full_path = os.path.join(root, file)\n",
    "            if f'train' in full_path:\n",
    "                name = file.split('_')[0]\n",
    "                if name in names:\n",
    "                    names[name] = names[name] + 1\n",
    "                else:\n",
    "                    names[name] = 0\n",
    "\n",
    "big_object_classes = []\n",
    "for name in names.keys():\n",
    "    if names[name] > 190: \n",
    "        print(f'{name} : {names[name]}')\n",
    "        big_object_classes.append(name)\n",
    "print(len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = PointCloudDataset2(\"../Data/ModelNet40\", 1024, 'train', object_classes = ['vase'] )\n",
    "t = PointCloudDataset(\"../Data/ModelNet40\", 1024, 'train', object_classes = ['vase'] )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zfogg\\OneDrive\\git\\Neural-Sign-Distance-Learning\\Helpers\\data2.py:67: RuntimeWarning: invalid value encountered in divide\n",
      "  scaled_points = ((points - min_coords) / range_coords) #scales points on range [0, 1]\n",
      "c:\\Users\\zfogg\\OneDrive\\git\\Neural-Sign-Distance-Learning\\Helpers\\data2.py:68: RuntimeWarning: invalid value encountered in cast\n",
      "  scaled_points = (scaled_points * (2**num_bits - 1)).astype(int) # scales points to [0, 2^num_bits]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1758\n",
      "7112\n"
     ]
    }
   ],
   "source": [
    "point_size = 1024\n",
    "# train_dataset = PointCloudDataset2(\"../Data/ModelNet40\", point_size, 'train', object_classes = big_object_classes )\n",
    "# train_loader = DataLoader(train_dataset, batch_size = 64, shuffle = True)\n",
    "test_dataset = PointCloudDataset2(\"../Data/ModelNet40\", point_size, 'test', object_classes=big_object_classes)\n",
    "# test_loader = DataLoader(test_dataset, batch_size = 32, shuffle = True)\n",
    "print(len(test_dataset))\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = 128, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 32, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(train_loader))['points'][0]\n",
    "\n",
    "cloud = pc.get_point_cloud(x)\n",
    "pc.visualize_point_cloud(cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1569\n"
     ]
    }
   ],
   "source": [
    "point_size = 1024\n",
    "# train_dataset = PointCloudDataset(\"../Data/ModelNet40\", point_size, 'train', object_classes = ['chair', 'sofa'])\n",
    "# train_loader = DataLoader(train_dataset, batch_size = 64, shuffle = True)\n",
    "test_dataset = PointCloudDataset(\"../Data/ModelNet40\", point_size, 'test', object_classes=['chair', 'sofa'])\n",
    "test_loader = DataLoader(test_dataset, batch_size = 32, shuffle = True)\n",
    "# print(len(test_dataset))\n",
    "print(len(train_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, point_size, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.point_size = point_size\n",
    "\n",
    "        self.l1 = nn.Linear(latent_dim, 768)\n",
    "        self.l2 = nn.Linear(768 , 1024)\n",
    "        self.l3 = nn.Linear(1024, 2048)\n",
    "        self.l4 = nn.Linear(2048, 3072)\n",
    "        self.l5 = nn.Linear(3072, point_size * 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.l1(x))\n",
    "        x = F.gelu(self.l2(x))\n",
    "        x = F.gelu(self.l3(x))\n",
    "        x = F.gelu(self.l4(x))\n",
    "        x = self.l5(x)\n",
    "        x = x.view(-1, self.point_size, 3)\n",
    "        return x    \n",
    "    \n",
    "class ConvEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, point_size, latent_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Blowup point representation from 3 to 32\n",
    "        self.conv1 = nn.Conv1d(3, 16, 1)\n",
    "        self.conv2 = nn.Conv1d(16, 32, 1)\n",
    "\n",
    "        # Points talk to each other wo/ downsampling \n",
    "        self.conv3 = nn.Conv1d(32, 32, kernel_size = 9, stride= 1, padding= 4)\n",
    "        self.conv4 = nn.Conv1d(32, 32, kernel_size = 9, stride = 1, padding = 4)\n",
    "        self.conv5 = nn.Conv1d(32, 32, kernel_size = 9, stride = 1, padding = 4)\n",
    "\n",
    "        # Downsampling \n",
    "        self.conv6 = nn.Conv1d(32, 32, kernel_size = 8, stride = 2, padding = 3)\n",
    "        self.conv7 = nn.Conv1d(32, 32, kernel_size = 8, stride = 2, padding = 3)\n",
    "        self.conv8 = nn.Conv1d(32, 32, kernel_size = 8, stride = 2, padding = 3)\n",
    "        self.conv9 = nn.Conv1d(32, 32, kernel_size = 8, stride = 2, padding = 3)\n",
    "\n",
    "        # Linear \n",
    "        self.lin1 = nn.Linear(2048, 1024)\n",
    "        self.lin2 = nn.Linear(1024, 768)\n",
    "        self.lin3 = nn.Linear(768, latent_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.gelu(self.conv1(x))\n",
    "        x = F.gelu(self.conv2(x))\n",
    "\n",
    "        x = F.gelu(self.conv3(x))\n",
    "        x = F.gelu(self.conv4(x))\n",
    "        x = F.gelu(self.conv5(x))\n",
    "\n",
    "        x = F.gelu(self.conv6(x))\n",
    "        x = F.gelu(self.conv7(x))\n",
    "        x = F.gelu(self.conv8(x))\n",
    "        x = F.gelu(self.conv9(x))\n",
    "\n",
    "        x = x.view(-1, 2048)\n",
    "\n",
    "        x = F.gelu(self.lin1(x))\n",
    "        x = F.gelu(self.lin2(x))\n",
    "        x = F.gelu(self.lin3(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, point_size, latent_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = ConvEncoder(point_size, latent_size)\n",
    "        self.decoder = ConvDecoder(point_size, latent_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_rep = self.encoder(x)\n",
    "        reconstructed_cloud = self.decoder(latent_rep)\n",
    "        return reconstructed_cloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointCloudAE(nn.Module):\n",
    "    def __init__(self, point_size, latent_size):\n",
    "        super(PointCloudAE, self).__init__()\n",
    "        \n",
    "        self.latent_size = latent_size\n",
    "        self.point_size = point_size\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, self.latent_size, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(self.latent_size)\n",
    "        \n",
    "        self.dec1 = nn.Linear(self.latent_size,256)\n",
    "        self.dec2 = nn.Linear(256,256)\n",
    "        self.dec3 = nn.Linear(256,self.point_size*3)\n",
    "\n",
    "    def encoder(self, x): \n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.bn3(self.conv3(x))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, self.latent_size)\n",
    "        return x\n",
    "    \n",
    "    def decoder(self, x):\n",
    "        x = F.relu(self.dec1(x))\n",
    "        x = F.relu(self.dec2(x))\n",
    "        x = self.dec3(x)\n",
    "        return x.view(-1, self.point_size, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train_loss: 0.00023084146219348026\n",
      "Epoch 100 Train_loss: 0.000217502844911492\n",
      "Epoch 200 Train_loss: 0.0002131844547958198\n",
      "Epoch 300 Train_loss: 0.0002102990555365316\n",
      "Epoch 400 Train_loss: 0.00021030257127925076\n",
      "Epoch 500 Train_loss: 0.00020553628432522103\n",
      "Epoch 600 Train_loss: 0.0002082797288577157\n",
      "Epoch 700 Train_loss: 0.00020384389155713673\n",
      "Epoch 800 Train_loss: 0.00020241157908458264\n",
      "Epoch 900 Train_loss: 0.00020250683115922253\n",
      "Epoch 1000 Train_loss: 0.0001977582440965555\n",
      "Epoch 1100 Train_loss: 0.00020172991472381082\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 33\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m40000\u001b[39m) :\n\u001b[0;32m     31\u001b[0m     startTime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 33\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#train one epoch, get the average loss\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     train_loss_list\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m     37\u001b[0m     epoch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m startTime\n",
      "Cell \u001b[1;32mIn[52], line 11\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpoints\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     output \u001b[38;5;241m=\u001b[39m net(x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m# transpose data for NumberxChannelxSize format\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# loss = F.mse_loss(x, output)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# net = PointCloudAE(1024, 256).to(device)\n",
    "# net = ConvAE(1024, 512).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.0001)\n",
    "\n",
    "def train_epoch():\n",
    "    epoch_loss = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x = data['points'].to(device)\n",
    "\n",
    "        output = net(x.permute(0,2,1)) # transpose data for NumberxChannelxSize format\n",
    "\n",
    "        # loss = F.mse_loss(x, output)\n",
    "        loss, _  = chamfer_distance(x, output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    if i == 0:\n",
    "        return epoch_loss / 1\n",
    "    return epoch_loss/i\n",
    "\n",
    "\n",
    "train_loss_list = []  \n",
    "test_loss_list = []  \n",
    "\n",
    "for i in range(40000) :\n",
    "\n",
    "    startTime = time.time()\n",
    "    \n",
    "    train_loss = train_epoch() #train one epoch, get the average loss\n",
    "    train_loss_list.append(train_loss)\n",
    "\n",
    "    \n",
    "    epoch_time = time.time() - startTime\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f'Epoch {i} Train_loss: {train_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'trained_models/furniture_10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, 'trained_models/furniture_model_weights_50k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('trained_models/furniture_model_weights_50k', weights_only = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net\n",
    "x = next(iter(train_loader))['points'][0]\n",
    "\n",
    "cloud = pc.get_point_cloud(x)\n",
    "pc.visualize_point_cloud(cloud)\n",
    "\n",
    "with torch.no_grad():\n",
    "    data = x.unsqueeze(0).permute(0,2,1).to(device)\n",
    "\n",
    "    rec_x = np.array(model(data)[0].to('cpu'))\n",
    "    cloud = pc.get_point_cloud(rec_x)\n",
    "    pc.visualize_point_cloud(cloud)\n",
    "# x = next(iter(train_loader))['points'][0]\n",
    "\n",
    "# cloud = pc.get_point_cloud(x)\n",
    "# pc.visualize_point_cloud(cloud)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     data = x.unsqueeze(0).permute(0,2,1).to(device)\n",
    "\n",
    "#     rec_x = np.array(model(data)[0].to('cpu'))\n",
    "#     cloud = pc.get_point_cloud(rec_x)\n",
    "#     pc.visualize_point_cloud(cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net\n",
    "x = next(iter(test_loader))['points'][0]\n",
    "\n",
    "cloud = pc.get_point_cloud(x)\n",
    "pc.visualize_point_cloud(cloud)\n",
    "\n",
    "with torch.no_grad():\n",
    "    data = x.unsqueeze(0).permute(0,2,1).to(device)\n",
    "\n",
    "    rec_x = np.array(model(data)[0].to('cpu'))\n",
    "    cloud = pc.get_point_cloud(rec_x)\n",
    "    pc.visualize_point_cloud(cloud)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-open3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
