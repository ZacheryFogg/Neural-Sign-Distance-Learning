{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import open3d as o3d\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataclasses import dataclass\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "from pytorch3d.loss import chamfer_distance\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import open3d as o3d\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataclasses import dataclass\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "from pytorch3d.loss import chamfer_distance\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from Helpers.data import PointCloudDataset\n",
    "import Helpers.PointCloudOpen3d as pc\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f'Using: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2316\n"
     ]
    }
   ],
   "source": [
    "point_size = 1024\n",
    "train_dataset = PointCloudDataset(\"../Data/ModelNet40\", point_size, 'train')\n",
    "train_loader = DataLoader(train_dataset, batch_size = 64, shuffle = True)\n",
    "# test_dataset = PointCloudDataset(\"../Data/ModelNet40\", point_size, 'test', object_classes=object_classes)\n",
    "# test_loader = DataLoader(test_dataset, batch_size = 32, shuffle = True)\n",
    "# print(len(test_dataset))\n",
    "print(len(train_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointCloudAE(nn.Module):\n",
    "    def __init__(self, point_size, latent_size):\n",
    "        super(PointCloudAE, self).__init__()\n",
    "        \n",
    "        self.latent_size = latent_size\n",
    "        self.point_size = point_size\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, self.latent_size, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(self.latent_size)\n",
    "        \n",
    "        self.dec1 = nn.Linear(self.latent_size,256)\n",
    "        self.dec2 = nn.Linear(256,256)\n",
    "        self.dec3 = nn.Linear(256,self.point_size*3)\n",
    "\n",
    "    def encoder(self, x): \n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.bn3(self.conv3(x))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, self.latent_size)\n",
    "        return x\n",
    "    \n",
    "    def decoder(self, x):\n",
    "        x = F.relu(self.dec1(x))\n",
    "        x = F.relu(self.dec2(x))\n",
    "        x = self.dec3(x)\n",
    "        return x.view(-1, self.point_size, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 31\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10000\u001b[39m) :\n\u001b[0;32m     29\u001b[0m     startTime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 31\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#train one epoch, get the average loss\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     train_loss_list\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m     35\u001b[0m     epoch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m startTime\n",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_epoch\u001b[39m():\n\u001b[0;32m      6\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtrain_loader\u001b[49m):\n\u001b[0;32m      8\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     10\u001b[0m         x \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoints\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "net = PointCloudAE(1024, 256).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.0001)\n",
    "\n",
    "def train_epoch():\n",
    "    epoch_loss = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x = data['points'].to(device)\n",
    "\n",
    "        output = net(x.permute(0,2,1)) # transpose data for NumberxChannelxSize format\n",
    "\n",
    "        loss = F.mse_loss(x, output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    if i == 0:\n",
    "        return epoch_loss / 1\n",
    "    return epoch_loss/i\n",
    "\n",
    "\n",
    "train_loss_list = []  \n",
    "test_loss_list = []  \n",
    "\n",
    "for i in range(10000) :\n",
    "\n",
    "    startTime = time.time()\n",
    "    \n",
    "    train_loss = train_epoch() #train one epoch, get the average loss\n",
    "    train_loss_list.append(train_loss)\n",
    "\n",
    "    \n",
    "    epoch_time = time.time() - startTime\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'trained_models/furniture_10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, 'trained_models/furniture_model_weights_50k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('trained_models/furniture_model_weights_50k', weights_only = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net\n",
    "x = next(iter(train_loader))['points'][0]\n",
    "\n",
    "cloud = pc.get_point_cloud(x)\n",
    "pc.visualize_point_cloud(cloud)\n",
    "\n",
    "with torch.no_grad():\n",
    "    data = x.unsqueeze(0).permute(0,2,1).to(device)\n",
    "\n",
    "    rec_x = np.array(model(data)[0].to('cpu'))\n",
    "    cloud = pc.get_point_cloud(rec_x)\n",
    "    pc.visualize_point_cloud(cloud)\n",
    "# x = next(iter(train_loader))['points'][0]\n",
    "\n",
    "# cloud = pc.get_point_cloud(x)\n",
    "# pc.visualize_point_cloud(cloud)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     data = x.unsqueeze(0).permute(0,2,1).to(device)\n",
    "\n",
    "#     rec_x = np.array(model(data)[0].to('cpu'))\n",
    "#     cloud = pc.get_point_cloud(rec_x)\n",
    "#     pc.visualize_point_cloud(cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net\n",
    "x = next(iter(test_loader))['points'][0]\n",
    "\n",
    "cloud = pc.get_point_cloud(x)\n",
    "pc.visualize_point_cloud(cloud)\n",
    "\n",
    "with torch.no_grad():\n",
    "    data = x.unsqueeze(0).permute(0,2,1).to(device)\n",
    "\n",
    "    rec_x = np.array(model(data)[0].to('cpu'))\n",
    "    cloud = pc.get_point_cloud(rec_x)\n",
    "    pc.visualize_point_cloud(cloud)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-open3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
