{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import open3d as o3d\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataclasses import dataclass\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from Helpers.data import PointCloudDataset\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f'Using: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9843\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mPointCloudDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../ModelNet40\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\zfogg\\OneDrive\\git\\Neural-Sign-Distance-Learning\\Helpers\\data.py:16\u001b[0m, in \u001b[0;36mPointCloudDataset.__init__\u001b[1;34m(self, base_dir, point_cloud_size, split)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_dir \u001b[38;5;241m=\u001b[39m base_dir\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_file_paths(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_dir)\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoint_clouds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_uniform_point_clouds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zfogg\\OneDrive\\git\\Neural-Sign-Distance-Learning\\Helpers\\data.py:50\u001b[0m, in \u001b[0;36mPointCloudDataset.get_uniform_point_clouds\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     48\u001b[0m point_clouds_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles: \n\u001b[1;32m---> 50\u001b[0m     mesh \u001b[38;5;241m=\u001b[39m \u001b[43mo3d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_triangle_mesh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \n\u001b[0;32m     52\u001b[0m         sampled_point_cloud \u001b[38;5;241m=\u001b[39m mesh\u001b[38;5;241m.\u001b[39msample_points_uniformly(number_of_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoint_cloud_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset = PointCloudDataset(\"../ModelNet40\", 5000, 'train')\n",
    "train_loader = DataLoader(train_dataset, batch_size = 16, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(config.input_dim, config.hidden_dim1)\n",
    "        self.fc2 = nn.Linear(config.hidden_dim1, config.hidden_dim2)\n",
    "        self.fc3 = nn.Linear(config.hidden_dim2, config.latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = F.gelu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class MLPDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(config.latent_dim, config.hidden_dim2)\n",
    "        self.fc2 = nn.Linear(config.hidden_dim2, config.hidden_dim1)\n",
    "        self.fc3 = nn.Linear(config.hidden_dim1, config.input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = F.gelu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = MLPEncoder(config)\n",
    "        self.decoder = MLPDecoder(config)\n",
    "\n",
    "    def forward(self,x):\n",
    "        latent_rep = self.encoder(x)\n",
    "        out = self.decoder(latent_rep)\n",
    "        return out\n",
    "\n",
    "\n",
    "# class Autoencoder(nn.Module):\n",
    "\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.fc = nn.Linear(config.input_dim, config.input_dim)\n",
    "\n",
    "#     def forward(self,x):\n",
    "#         y = self.fc(x)\n",
    "#         return x + (.00001 * y)\n",
    "    \n",
    "\n",
    "class PointCloudAutoencoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(PointCloudAutoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(config.input_dim, config.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.hidden_dim, config.latent_dim)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(config.latent_dim, config.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.hidden_dim, config.input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   Epoch Loss: 2.3480648486699917e+19\n",
      "Epoch 1   Epoch Loss: 2.495094256946952e+20\n",
      "Epoch 2   Epoch Loss: 2.1314188879035213e+20\n",
      "Epoch 3   Epoch Loss: 7.413677845910648e+19\n",
      "Epoch 4   Epoch Loss: 2.2948326830167376e+19\n",
      "Epoch 5   Epoch Loss: 2.0969642647735427e+19\n",
      "Epoch 6   Epoch Loss: 1.798323757158154e+19\n",
      "Epoch 7   Epoch Loss: 2.57058508589087e+19\n",
      "Epoch 8   Epoch Loss: 7.426546715074205e+18\n",
      "Epoch 9   Epoch Loss: 9.568644576636707e+18\n",
      "Epoch 10  Epoch Loss: 1.0529830274308594e+19\n",
      "Epoch 11  Epoch Loss: 1.2385103378147912e+19\n",
      "Epoch 12  Epoch Loss: 8.210757999839613e+18\n",
      "Epoch 13  Epoch Loss: 6.844340003392166e+18\n",
      "Epoch 14  Epoch Loss: 1.3062039865639199e+19\n",
      "Epoch 15  Epoch Loss: 2.0046769900805444e+19\n",
      "Epoch 16  Epoch Loss: 1.7305529961227743e+19\n",
      "Epoch 17  Epoch Loss: 5.178540398903014e+18\n",
      "Epoch 18  Epoch Loss: 6.932961992673825e+18\n",
      "Epoch 19  Epoch Loss: 7.116343701280102e+18\n",
      "Epoch 20  Epoch Loss: 9.438354456650093e+18\n",
      "Epoch 21  Epoch Loss: 1.1172786477994236e+19\n",
      "Epoch 22  Epoch Loss: 7.367429962727038e+18\n",
      "Epoch 23  Epoch Loss: 9.179363748446108e+18\n",
      "Epoch 24  Epoch Loss: 4.817524292935532e+18\n",
      "Epoch 25  Epoch Loss: 3.8274181209359027e+18\n",
      "Epoch 26  Epoch Loss: 7.021049378927096e+18\n",
      "Epoch 27  Epoch Loss: 1.7811070610238872e+19\n",
      "Epoch 28  Epoch Loss: 9.567519177249847e+18\n",
      "Epoch 29  Epoch Loss: 4.824173096963501e+18\n",
      "Epoch 30  Epoch Loss: 7.8800667667627e+18\n",
      "Epoch 31  Epoch Loss: 4.873070592927674e+18\n",
      "Epoch 32  Epoch Loss: 1.8163848038734893e+19\n",
      "Epoch 33  Epoch Loss: 1.1101227919469076e+19\n",
      "Epoch 34  Epoch Loss: 1.2468223897672133e+19\n",
      "Epoch 35  Epoch Loss: 6.270452581285461e+18\n",
      "Epoch 36  Epoch Loss: 4.5434449577024323e+18\n",
      "Epoch 37  Epoch Loss: 5.433361320873949e+18\n",
      "Epoch 38  Epoch Loss: 7.105597574662141e+18\n",
      "Epoch 39  Epoch Loss: 5.446803728506625e+18\n",
      "Epoch 40  Epoch Loss: 8.283815758956127e+18\n",
      "Epoch 41  Epoch Loss: 1.0186839560813949e+19\n",
      "Epoch 42  Epoch Loss: 1.9961104482203206e+19\n",
      "Epoch 43  Epoch Loss: 8.90893468022293e+18\n",
      "Epoch 44  Epoch Loss: 8.697940401006516e+18\n",
      "Epoch 45  Epoch Loss: 9.275007207088509e+18\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 56\u001b[0m\n\u001b[1;32m     52\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     54\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(pred, x)\n\u001b[0;32m---> 56\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     60\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch-gpu/lib/python3.13/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_tensor_str\u001b[38;5;241m.\u001b[39m_str(\u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents)\n\u001b[0;32m--> 525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    527\u001b[0m ):\n\u001b[1;32m    528\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \n\u001b[1;32m    530\u001b[0m \u001b[38;5;124;03m    The graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m            used to compute the :attr:`tensors`.\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "point_cloud_size = 5000 \n",
    "\n",
    "\n",
    "# @dataclass \n",
    "# class MLPAutoEncoderConfig:\n",
    "#     input_dim = point_cloud_size * 3\n",
    "#     hidden_dim1 = 3072\n",
    "#     hidden_dim2 = 1048\n",
    "#     latent_dim = 512\n",
    "\n",
    "# @dataclass \n",
    "# class MLPAutoEncoderConfig:\n",
    "#     input_dim = point_cloud_size * 3\n",
    "#     hidden_dim1 = point_cloud_size * 3\n",
    "#     hidden_dim2 = point_cloud_size * 3\n",
    "#     latent_dim = point_cloud_size * 3\n",
    "\n",
    "@dataclass \n",
    "class MLPAutoEncoderConfig:\n",
    "    input_dim = point_cloud_size * 3\n",
    "    hidden_dim = 2048\n",
    "    latent_dim = 512\n",
    "\n",
    "\n",
    "config = MLPAutoEncoderConfig()\n",
    "\n",
    "# model = Autoencoder(config).to(device)\n",
    "model = PointCloudAutoencoder(config).to(device)\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr= 1e-4)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "report_rate = 600\n",
    "\n",
    "s= time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    running_loss = 0 \n",
    "\n",
    "    batch_count = 0 \n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "\n",
    "        x = data['points']\n",
    "        x = x.view(x.shape[0], -1).to(device)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "\n",
    "        pred = model(x)\n",
    "\n",
    "        loss = F.mse_loss(pred, x)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        batch_count +=1\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch {epoch:<3} Epoch Loss: {running_loss / batch_count}')\n",
    "\n",
    "        # if i % report_rate == report_rate - 1:\n",
    "        #     print(f'Batch {i:<3} Running Loss: {running_loss / report_rate}')\n",
    "        #     running_loss = 0\n",
    "\n",
    "print(time.time() - s)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 15000])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(x.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
