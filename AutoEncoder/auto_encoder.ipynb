{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mopen3d\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mo3d\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import open3d as o3d\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataclasses import dataclass\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "from pytorch3d.loss import chamfer_distance\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from Helpers.data import PointCloudDataset\n",
    "import Helpers.PointCloudOpen3d as pc\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f'Using: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "\n",
    "class PointCloudDataset(Dataset):\n",
    "    def __init__(self,base_dir, point_cloud_size = 5000, split = 'train', object_classes = None):\n",
    "        \n",
    "        self.point_cloud_size = point_cloud_size\n",
    "        self.point_clouds = None\n",
    "        self.split = split\n",
    "        self.base_dir = base_dir\n",
    "        self.object_classes = object_classes\n",
    "\n",
    "        self.files = self.get_file_paths()\n",
    "        self.point_clouds = self.get_uniform_point_clouds()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.point_clouds)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"points\": self.point_clouds[idx],\n",
    "            \"filename\" : self.files[idx]\n",
    "        }\n",
    "    \n",
    "    def get_file_paths(self):\n",
    "        '''\n",
    "        Return list of all filepaths in ModelNet40 that are part of split (train or test)\n",
    "        If self.object_classes is populated with class names, then only files in those classes will be returned\n",
    "        '''\n",
    "        file_paths = []\n",
    "        for root, _, files in os.walk(self.base_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.off'):\n",
    "                    full_path = os.path.join(root, file)\n",
    "                    if f'{self.split}' in full_path:\n",
    "                        if self.object_classes is not None:\n",
    "                            if file.split('_')[0] in self.object_classes:\n",
    "                                file_paths.append(full_path)\n",
    "                        else: \n",
    "                            file_paths.append(full_path)\n",
    "        return file_paths\n",
    "    \n",
    "    \n",
    "    def get_uniform_point_clouds(self):\n",
    "        '''\n",
    "        Return a tensor that is all point clouds of fixed size\n",
    "        '''\n",
    "        \n",
    "        point_clouds_list = []\n",
    "        for file in self.files: \n",
    "            mesh = o3d.io.read_triangle_mesh(file)\n",
    "            try: \n",
    "                sampled_point_cloud = mesh.sample_points_uniformly(number_of_points = self.point_cloud_size)\n",
    "                cloud = torch.tensor(np.asanyarray(sampled_point_cloud.points) ,dtype = torch.float32)\n",
    "                point_clouds_list.append(F.normalize(cloud, dim = 0))\n",
    "            except RuntimeError: # Some .OFF files are damaged, run repair script\n",
    "                print(f'Damaged file: {file}')\n",
    "\n",
    "        return point_clouds_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PointCloudDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# train_dataset_full = PointCloudDataset(\"../ModelNet40\", 5000, 'train')\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# train_loader = DataLoader(train_dataset_full, batch_size = 32, shuffle = False)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mPointCloudDataset\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../ModelNet40\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m5000\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, object_classes\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mairplane\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PointCloudDataset' is not defined"
     ]
    }
   ],
   "source": [
    "# train_dataset_full = PointCloudDataset(\"../ModelNet40\", 5000, 'train')\n",
    "# train_loader = DataLoader(train_dataset_full, batch_size = 32, shuffle = False)\n",
    "train_dataset = PointCloudDataset(\"../ModelNet40\", 5000, 'train', object_classes= ['airplane'])\n",
    "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3746, 1024, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pc_array = np.load(\"../chair_set.npy\")\n",
    "print(pc_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(config.input_dim, config.hidden_dim1)\n",
    "        self.fc2 = nn.Linear(config.hidden_dim1, config.hidden_dim2)\n",
    "        self.fc3 = nn.Linear(config.hidden_dim2, config.latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = F.gelu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class MLPDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(config.latent_dim, config.hidden_dim2)\n",
    "        self.fc2 = nn.Linear(config.hidden_dim2, config.hidden_dim1)\n",
    "        self.fc3 = nn.Linear(config.hidden_dim1, config.input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = F.gelu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = MLPEncoder(config)\n",
    "        self.decoder = MLPDecoder(config)\n",
    "\n",
    "    def forward(self,x):\n",
    "        latent_rep = self.encoder(x)\n",
    "        out = self.decoder(latent_rep)\n",
    "        return out\n",
    "\n",
    "\n",
    "# class Autoencoder(nn.Module):\n",
    "\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.fc = nn.Linear(config.input_dim, config.input_dim)\n",
    "\n",
    "#     def forward(self,x):\n",
    "#         y = self.fc(x)\n",
    "#         return x + (.00001 * y)\n",
    "    \n",
    "\n",
    "class PointCloudAutoencoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(PointCloudAutoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(config.input_dim, config.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.hidden_dim, config.latent_dim)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(config.latent_dim, config.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.hidden_dim, config.input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PointCloudAE(nn.Module):\n",
    "    def __init__(self, point_size, latent_size):\n",
    "        super(PointCloudAE, self).__init__()\n",
    "        \n",
    "        self.latent_size = latent_size\n",
    "        self.point_size = point_size\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, self.latent_size, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(self.latent_size)\n",
    "        \n",
    "        self.dec1 = nn.Linear(self.latent_size,256)\n",
    "        self.dec2 = nn.Linear(256,256)\n",
    "        self.dec3 = nn.Linear(256,self.point_size*3)\n",
    "\n",
    "    def encoder(self, x): \n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.bn3(self.conv3(x))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, self.latent_size)\n",
    "        return x\n",
    "    \n",
    "    def decoder(self, x):\n",
    "        x = F.relu(self.dec1(x))\n",
    "        x = F.relu(self.dec2(x))\n",
    "        x = self.dec3(x)\n",
    "        return x.view(-1, self.point_size, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "# class PointCloudAE(nn.Module):\n",
    "#     def __init__(self, point_size, latent_size):\n",
    "#         super(PointCloudAE, self).__init__()\n",
    "        \n",
    "#         self.latent_size = latent_size\n",
    "#         self.point_size = point_size\n",
    "        \n",
    "#         self.conv1 = torch.nn.Conv1d(3, 1024, 1)\n",
    "#         self.conv2 = torch.nn.Conv1d(1024, 768, 1)\n",
    "#         self.conv3 = torch.nn.Conv1d(768, self.latent_size, 1)\n",
    "#         self.bn1 = nn.BatchNorm1d(1024)\n",
    "#         self.bn2 = nn.BatchNorm1d(768)\n",
    "#         self.bn3 = nn.BatchNorm1d(self.latent_size)\n",
    "        \n",
    "#         self.dec1 = nn.Linear(self.latent_size,1024)\n",
    "#         self.dec2 = nn.Linear(1024,2048)\n",
    "#         self.dec3 = nn.Linear(2048,self.point_size*3)\n",
    "\n",
    "#     def encoder(self, x): \n",
    "#         x = F.relu(self.bn1(self.conv1(x)))\n",
    "#         x = F.relu(self.bn2(self.conv2(x)))\n",
    "#         x = self.bn3(self.conv3(x))\n",
    "#         x = torch.max(x, 2, keepdim=True)[0]\n",
    "#         x = x.view(-1, self.latent_size)\n",
    "#         return x\n",
    "    \n",
    "#     def decoder(self, x):\n",
    "#         x = F.relu(self.dec1(x))\n",
    "#         x = F.relu(self.dec2(x))\n",
    "#         x = self.dec3(x)\n",
    "\n",
    "#         return x.view(-1, 3, self.point_size)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.encoder(x)\n",
    "#         x = self.decoder(x)\n",
    "#         return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(train_loader))['points']\n",
    "cloud = pc.get_point_cloud(x[2])\n",
    "pc.visualize_point_cloud(cloud)\n",
    "# means0 = x[:,:,0].mean(dim = 0, keepdim = True)\n",
    "# stds0 = x[:,:,0].std(dim = 0, keepdim = True)\n",
    "\n",
    "# x[:,:,0] = (x[:,:,0] - means0) / 1\n",
    "\n",
    "# means0 = x[:,:,1].mean(dim = 0, keepdim = True)\n",
    "# stds0 = x[:,:,1].std(dim = 0, keepdim = True)\n",
    "\n",
    "# x[:,:,1] = (x[:,:,1] - means0) / 1\n",
    "\n",
    "\n",
    "# means0 = x[:,:,2].mean(dim = 0, keepdim = True)\n",
    "# stds0 = x[:,:,2].std(dim = 0, keepdim = True)\n",
    "\n",
    "# x[:,:,2] = (x[:,:,2] - means0) / 1\n",
    "\n",
    "# # mean = x.mean(dim = 1, keepdim = True)\n",
    "# # std = x.std(dim = 1, keepdim = True)\n",
    "\n",
    "# # x = (x - mean) / std\n",
    "\n",
    "# cloud = pc.get_point_cloud(x[2])\n",
    "# pc.visualize_point_cloud(cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "\n",
    "class ReadDataset(Dataset):\n",
    "    def __init__(self,  source):\n",
    "     \n",
    "        self.data = torch.from_numpy(source).float()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "def RandomSplit(datasets, train_set_percentage):\n",
    "    lengths = [int(len(datasets)*train_set_percentage), len(datasets)-int(len(datasets)*train_set_percentage)]\n",
    "    return random_split(datasets, lengths)\n",
    "\n",
    "def GetDataLoaders(npArray, batch_size, train_set_percentage = 0.9, shuffle=True, num_workers=0, pin_memory=True):\n",
    "    \n",
    "    \n",
    "    pc = ReadDataset(npArray)\n",
    "\n",
    "    train_set, test_set = RandomSplit(pc, train_set_percentage)\n",
    "\n",
    "    train_loader = DataLoader(train_set, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size, pin_memory=pin_memory)\n",
    "    test_loader = DataLoader(test_set, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size, pin_memory=pin_memory)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "pc_array = np.load(\"../chair_set.npy\")\n",
    "\n",
    "train_loader, test_loader = GetDataLoaders(npArray=pc_array, batch_size= 64)\n",
    "\n",
    "# cloud = pc.get_point_cloud(pc_array[0])\n",
    "# pc.visualize_point_cloud(cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PointCloudAE(1024,768).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9   Epoch Loss: 43.05035206056991\n",
      "Epoch 19  Epoch Loss: 42.993158088540135\n",
      "Epoch 29  Epoch Loss: 42.90096081427808\n",
      "Epoch 39  Epoch Loss: 42.92971391497918\n",
      "Epoch 49  Epoch Loss: 42.90210435975273\n",
      "Epoch 59  Epoch Loss: 42.79433642693286\n",
      "Epoch 69  Epoch Loss: 42.81280323244491\n",
      "Epoch 79  Epoch Loss: 42.805153684796025\n",
      "Epoch 89  Epoch Loss: 42.76938010161778\n",
      "Epoch 99  Epoch Loss: 42.729430000737025\n",
      "Epoch 109 Epoch Loss: 42.71458614997144\n",
      "Epoch 119 Epoch Loss: 42.66502387568636\n",
      "Epoch 129 Epoch Loss: 42.6709646548865\n",
      "Epoch 139 Epoch Loss: 42.66444728059589\n",
      "Epoch 149 Epoch Loss: 42.69248393796525\n",
      "Epoch 159 Epoch Loss: 42.601971752238725\n",
      "Epoch 169 Epoch Loss: 42.58128601650022\n",
      "Epoch 179 Epoch Loss: 42.516199507803286\n",
      "Epoch 189 Epoch Loss: 42.511252133351455\n",
      "Epoch 199 Epoch Loss: 42.47107833286501\n",
      "Epoch 209 Epoch Loss: 42.44439481339365\n",
      "Epoch 219 Epoch Loss: 42.43839825324292\n",
      "Epoch 229 Epoch Loss: 42.411283241128025\n",
      "Epoch 239 Epoch Loss: 42.42417533442659\n",
      "Epoch 249 Epoch Loss: 42.34686624778892\n",
      "Epoch 259 Epoch Loss: 42.347766156466506\n",
      "Epoch 269 Epoch Loss: 42.31417587568175\n",
      "Epoch 279 Epoch Loss: 42.27361779842737\n",
      "Epoch 289 Epoch Loss: 42.222911330888856\n",
      "Epoch 299 Epoch Loss: 42.20846140159751\n",
      "Epoch 309 Epoch Loss: 42.22406070637253\n",
      "Epoch 319 Epoch Loss: 42.21421252556567\n",
      "Epoch 329 Epoch Loss: 42.19586044887327\n",
      "Epoch 339 Epoch Loss: 42.147095878169225\n",
      "Epoch 349 Epoch Loss: 42.10489460207381\n",
      "Epoch 359 Epoch Loss: 42.06845790935012\n",
      "Epoch 369 Epoch Loss: 42.05423254336951\n",
      "Epoch 379 Epoch Loss: 42.03611798556346\n",
      "Epoch 389 Epoch Loss: 42.01311406549418\n",
      "Epoch 399 Epoch Loss: 42.00762752317033\n",
      "Epoch 409 Epoch Loss: 42.014168865275835\n",
      "Epoch 419 Epoch Loss: 41.973549249037255\n",
      "Epoch 429 Epoch Loss: 41.92762648384526\n",
      "Epoch 439 Epoch Loss: 41.908384647009505\n",
      "Epoch 449 Epoch Loss: 41.938081201517356\n",
      "Epoch 459 Epoch Loss: 41.8749992802458\n",
      "Epoch 469 Epoch Loss: 41.94094078495817\n",
      "Epoch 479 Epoch Loss: 41.855142845297756\n",
      "Epoch 489 Epoch Loss: 41.86117042685455\n",
      "Epoch 499 Epoch Loss: 41.85175093164984\n",
      "Epoch 509 Epoch Loss: 41.89942046831239\n",
      "Epoch 519 Epoch Loss: 41.80912492860038\n",
      "Epoch 529 Epoch Loss: 41.818181379786076\n",
      "Epoch 539 Epoch Loss: 41.76097646749245\n",
      "Epoch 549 Epoch Loss: 41.75429153442383\n",
      "Epoch 559 Epoch Loss: 41.73622023384526\n",
      "Epoch 569 Epoch Loss: 41.77589949122015\n",
      "Epoch 579 Epoch Loss: 41.77100156388193\n",
      "Epoch 589 Epoch Loss: 41.74334486475531\n",
      "Epoch 599 Epoch Loss: 41.69892300299878\n",
      "Epoch 609 Epoch Loss: 41.7122394633743\n",
      "Epoch 619 Epoch Loss: 41.69486344535396\n",
      "Epoch 629 Epoch Loss: 41.66379935786409\n",
      "Epoch 639 Epoch Loss: 41.694677460868405\n",
      "Epoch 649 Epoch Loss: 41.667002192083395\n",
      "Epoch 659 Epoch Loss: 41.64003329007131\n",
      "Epoch 669 Epoch Loss: 41.61698244202812\n",
      "Epoch 679 Epoch Loss: 41.619097583698775\n",
      "Epoch 689 Epoch Loss: 41.60025852131394\n",
      "Epoch 699 Epoch Loss: 41.60772885016675\n",
      "Epoch 709 Epoch Loss: 41.58196668804816\n",
      "Epoch 719 Epoch Loss: 41.54178417853589\n",
      "Epoch 729 Epoch Loss: 41.56588802697524\n",
      "Epoch 739 Epoch Loss: 41.618181264625406\n",
      "Epoch 749 Epoch Loss: 41.5396449250995\n",
      "Epoch 759 Epoch Loss: 41.541637276703455\n",
      "Epoch 769 Epoch Loss: 41.510716132397924\n",
      "Epoch 779 Epoch Loss: 41.47137990987526\n",
      "Epoch 789 Epoch Loss: 41.54291009003261\n",
      "Epoch 799 Epoch Loss: 41.509816223720335\n",
      "Epoch 809 Epoch Loss: 41.48477597506541\n",
      "Epoch 819 Epoch Loss: 41.47960144618772\n",
      "Epoch 829 Epoch Loss: 41.49785095790647\n",
      "Epoch 839 Epoch Loss: 41.45083179114\n",
      "Epoch 849 Epoch Loss: 41.45637447429153\n",
      "Epoch 859 Epoch Loss: 41.44323420974443\n",
      "Epoch 869 Epoch Loss: 41.42343247611568\n",
      "Epoch 879 Epoch Loss: 41.47389890562813\n",
      "Epoch 889 Epoch Loss: 41.40336774430185\n",
      "Epoch 899 Epoch Loss: 41.402651444920956\n",
      "Epoch 909 Epoch Loss: 41.3902734720482\n",
      "Epoch 919 Epoch Loss: 41.42152304019568\n",
      "Epoch 929 Epoch Loss: 41.409786224365234\n",
      "Epoch 939 Epoch Loss: 41.45087274515404\n",
      "Epoch 949 Epoch Loss: 41.40609057444446\n",
      "Epoch 959 Epoch Loss: 41.496175945929764\n",
      "Epoch 969 Epoch Loss: 41.34547316353276\n",
      "Epoch 979 Epoch Loss: 41.35519761859246\n",
      "Epoch 989 Epoch Loss: 41.342492949287845\n",
      "Epoch 999 Epoch Loss: 41.391298042153416\n"
     ]
    }
   ],
   "source": [
    "point_cloud_size = 5000 \n",
    "\n",
    "\n",
    "# @dataclass \n",
    "# class MLPAutoEncoderConfig:\n",
    "#     input_dim = point_cloud_size * 3\n",
    "#     hidden_dim1 = 3072\n",
    "#     hidden_dim2 = 1048\n",
    "#     latent_dim = 512\n",
    "\n",
    "# @dataclass \n",
    "# class MLPAutoEncoderConfig:\n",
    "#     input_dim = point_cloud_size * 3\n",
    "#     hidden_dim1 = point_cloud_size * 3\n",
    "#     hidden_dim2 = point_cloud_size * 3\n",
    "#     latent_dim = point_cloud_size * 3\n",
    "\n",
    "# @dataclass \n",
    "# class MLPAutoEncoderConfig:\n",
    "#     input_dim = point_cloud_size * 3\n",
    "#     hidden_dim = 2048\n",
    "#     latent_dim = 512\n",
    "\n",
    "\n",
    "# config = MLPAutoEncoderConfig()\n",
    "\n",
    "# model = Autoencoder(config).to(device)\n",
    "# model = PointCloudAutoencoder(config).to(device)\n",
    "# model = PointCloudAE(5000,256).to(device)\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr= 0.0005)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "\n",
    "report_rate = 600\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    running_loss = 0 \n",
    "\n",
    "    batch_count = 0 \n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        x = data.to(device).permute(0,2,1)\n",
    "        # x = data['points'].to(device).permute(0,2,1)\n",
    "        # x = F.normalize(x, dim = 2)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "\n",
    "        pred = model(x)\n",
    "        pred = pred.permute(0,2,1)\n",
    "        # print(x.shape)\n",
    "        # print(pred.shape)\n",
    "        # loss = F.mse_loss(pred, x)\n",
    "        # print(x.shape)\n",
    "        # print(pred.shape)\n",
    "        loss, _ = chamfer_distance(x, pred) \n",
    "\n",
    "        # cloud = pc.get_point_cloud(x[1].T.to('cpu'))\n",
    "\n",
    "        # pc.visualize_point_cloud(cloud)\n",
    "        # p = pred[1].T.to('cpu').detach()\n",
    "\n",
    "        # cloud = pc.get_point_cloud(p)\n",
    "        # pc.visualize_point_cloud(cloud)\n",
    "\n",
    "    \n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        batch_count +=1\n",
    "\n",
    "    if epoch % 10 == 9:\n",
    "        print(f'Epoch {epoch:<3} Epoch Loss: {running_loss / batch_count}')\n",
    "        \n",
    "\n",
    "        # if i % report_rate == report_rate - 1:\n",
    "        #     print(f'Batch {i:<3} Running Loss: {running_loss / report_rate}')\n",
    "        #     running_loss = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "x = next(iter(train_loader))[1]\n",
    "x = F.normalize(x, dim = 0)\n",
    "\n",
    "print(type(x))\n",
    "cloud = pc.get_point_cloud(x)\n",
    "pc.visualize_point_cloud(cloud)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = x.T.unsqueeze(0).to(device)\n",
    "    rec_x = np.array(model(x)[0].to('cpu'))\n",
    "    cloud = pc.get_point_cloud(rec_x)\n",
    "    pc.visualize_point_cloud(cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 15000])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(x.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'human_auto_encoder')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "6ba0af6ca18a87fd9d9a83a0a83c49166e2958272ee4ed49414730c2cc63cbf0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
