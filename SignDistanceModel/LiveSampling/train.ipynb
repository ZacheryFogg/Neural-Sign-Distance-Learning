{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import open3d as o3d\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from dataclasses import dataclass\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from pytorch3d.loss import chamfer_distance\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "import AutoEncoder.autoencoders as ae\n",
    "from AutoEncoder.encoder import PointCloudAutoEncoder\n",
    "from Helpers.data import PointCloudDataset\n",
    "import Helpers.PointCloudOpen3d as pc\n",
    "from Helpers.nsdf_live_sampler import sample_sdf\n",
    "from SignDistanceModel.sdf_models import SDFRegressionModel\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f'Using: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_1024 = PointCloudDataset(\"../Data/ModelNet40\", 1024, 'train', object_classes =  [\"sofa\"] )\n",
    "test_dataset_1024 = PointCloudDataset(\"../Data/ModelNet40\", 1024, 'test', object_classes =  [\"sofa\"])\n",
    "\n",
    "test_size = len(test_dataset_1024)\n",
    "split_idx = test_size // 2\n",
    "indices = list(range(test_size))\n",
    "val_dataset_1024 = Subset(test_dataset_1024, indices[:split_idx])\n",
    "test_dataset_1024 = Subset(test_dataset_1024, indices[split_idx:])\n",
    "\n",
    "train_loader_1024= DataLoader(train_dataset_1024, batch_size = 64, shuffle = True)\n",
    "val_loader_1024 = DataLoader(val_dataset_1024, batch_size = 128, shuffle = False)\n",
    "test_loader_1024 = DataLoader(test_dataset_1024, batch_size = 128, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_3072 = PointCloudDataset(\"../Data/ModelNet40\", 3072, 'train', object_classes = None )\n",
    "test_dataset_3072 = PointCloudDataset(\"../Data/ModelNet40\", 3072, 'test', object_classes = None)\n",
    "\n",
    "test_size = len(test_dataset_3072)\n",
    "split_idx = test_size // 2\n",
    "indices = list(range(test_size))\n",
    "val_dataset_3072 = Subset(test_dataset_3072, indices[:split_idx])\n",
    "test_dataset_3072 = Subset(test_dataset_3072, indices[split_idx:])\n",
    "\n",
    "train_loader_3072 = DataLoader(train_dataset_3072, batch_size = 64, shuffle = True)\n",
    "val_loader_3072 = DataLoader(val_dataset_3072, batch_size = 128, shuffle = False)\n",
    "test_loader_3072 = DataLoader(test_dataset_3072, batch_size = 128, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_dataset_1024 , 'train_dataset_1024.pt')\n",
    "torch.save(test_dataset_1024, 'test_dataset_1024.pt')\n",
    "torch.save(val_dataset_1024, 'val_dataset_1024.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_dataset_3072 , 'train_dataset_3072.pt')\n",
    "torch.save(test_dataset_3072, 'test_dataset_3072.pt')\n",
    "torch.save(val_dataset_3072, 'val_dataset_3072.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_1024 = torch.load('val_dataset_1024.pt', weights_only = False)\n",
    "test_dataset_1024 = torch.load('test_dataset_1024.pt', weights_only = False)\n",
    "train_dataset_1024 = torch.load('train_dataset_1024.pt', weights_only = False)\n",
    "\n",
    "train_loader_1024= DataLoader(train_dataset_1024, batch_size = 64, shuffle = True)\n",
    "val_loader_1024 = DataLoader(val_dataset_1024, batch_size = 64, shuffle = False)\n",
    "test_loader_1024 = DataLoader(test_dataset_1024, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_3072 = torch.load('val_dataset_3072.pt', weights_only = False)\n",
    "test_dataset_3072 = torch.load('test_dataset_3072.pt', weights_only = False)\n",
    "train_dataset_3072 = torch.load('train_dataset_3072.pt', weights_only = False)\n",
    "\n",
    "train_loader_3072 = DataLoader(train_dataset_3072, batch_size = 64, shuffle = True)\n",
    "val_loader_3072 = DataLoader(val_dataset_3072, batch_size = 64, shuffle = False)\n",
    "test_loader_3072 = DataLoader(test_dataset_3072, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvAE_6800T(\n",
      "  (encoder): ConvEncoder_6800T(\n",
      "    (conv1): Conv1d(3, 32, kernel_size=(1,), stride=(1,))\n",
      "    (conv2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "    (conv3): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "    (conv4): Conv1d(32, 16, kernel_size=(8,), stride=(2,), padding=(3,))\n",
      "    (conv5): Conv1d(16, 16, kernel_size=(8,), stride=(2,), padding=(3,))\n",
      "    (conv6): Conv1d(16, 16, kernel_size=(8,), stride=(2,), padding=(3,))\n",
      "    (lin1): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (lin2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  )\n",
      "  (decoder): ConvDecoder(\n",
      "    (l1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (l2): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (l3): Linear(in_features=2048, out_features=3072, bias=True)\n",
      "    (l4): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = ae.ConvAE_6800T(1024, 256)\n",
    "model.load_state_dict(torch.load('../Data/trained_autoencoders/1024_256/Conv_6800T', weights_only=True))\n",
    "print(model.to('cpu'))\n",
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_random_reconstruction(model, x, device, transpose_out = False, show_org = True):\n",
    "    '''\n",
    "    Pick a random cloud from the dataset and show what it looks like before and after autoencoder reconstruction \n",
    "    First window is original point cloud \n",
    "    Second window is recontructed point cloud\n",
    "    '''\n",
    "\n",
    "    def show_cloud(x):\n",
    "        pc.visualize_point_cloud(pc.get_point_cloud(x.cpu().numpy().astype(np.float64)))\n",
    "\n",
    "    \n",
    "    if show_org:\n",
    "        show_cloud(x)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x = x.unsqueeze(0).permute(0,2,1).to(device)\n",
    "        \n",
    "        rec_x = model(x)[0].to('cpu')\n",
    "        \n",
    "        if transpose_out:\n",
    "            rec_x = rec_x.T\n",
    "        \n",
    "        rec_x = rec_x\n",
    "\n",
    "        show_cloud(rec_x)\n",
    "\n",
    "#visualize_random_reconstruction(model, x, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/ModelNet40/sofa/test/sofa_0706.off\n"
     ]
    }
   ],
   "source": [
    "x = next(iter(val_loader_1024))\n",
    "visualize_random_reconstruction(model, x['points'][0].to('cpu'), 'cpu')\n",
    "print(x[\"filename\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_points_batch(point_cloud, encoder_model, num_points=1000, batches=64):\n",
    "    sampled_points = None\n",
    "    sampled_labels = None\n",
    "    corr_neural_encoding = None\n",
    "    filenames = point_cloud[\"filename\"]\n",
    "    for i in range(len(filenames)):\n",
    "        if batches <= i:\n",
    "            break\n",
    "        filename_split = filenames[i].split('/')\n",
    "        filename_split[2] = \"vdbs\"\n",
    "        filename_split[-1]  = filename_split[-1][:-3] + \"vdb\" \n",
    "        vdb_filename = \"\".join([i + '/' for i in filename_split])[:-1]\n",
    "        points, labels = sample_sdf(vdb_filename, num_points, device=device)\n",
    "        x = point_cloud['points'].to(device)[i].unsqueeze(0)\n",
    "        y = x.permute(0,2,1)\n",
    "        with torch.no_grad():\n",
    "            neural_encoding = encoder_model.encoder(y)\n",
    "        if sampled_points == None:\n",
    "            sampled_points = points\n",
    "            sampled_labels = labels\n",
    "            corr_neural_encoding = neural_encoding.repeat(num_points, 1)\n",
    "        else:\n",
    "            sampled_points = torch.cat((sampled_points, points), dim=0)\n",
    "            sampled_labels = torch.cat((sampled_labels, labels), dim=0)\n",
    "            corr_neural_encoding = torch.cat((corr_neural_encoding, neural_encoding.repeat(num_points, 1)))\n",
    "    return sampled_points, sampled_labels, corr_neural_encoding\n",
    "\n",
    "def train_one_mesh(filename, model, neural_encoding, optimizer, live_sampling=True, num_points=1000, num_epochs = 10):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    criterion = nn.MSELoss()\n",
    "    filename_split = filename.split('/')\n",
    "    filename_split[2] = \"vdbs\"\n",
    "    filename_split[-1]  = filename_split[-1][:-3] + \"vdb\" \n",
    "    vdb_filename = \"\".join([i + '/' for i in filename_split])[:-1]\n",
    "    neural_encoding = neural_encoding.repeat(num_points, 1)\n",
    "    running_acc = 0\n",
    "    running_loss = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        points, labels = sample_sdf(vdb_filename, num_points, device=device)\n",
    "        perm = torch.randperm(num_points)\n",
    "        points, labels = points[perm], labels[perm]\n",
    "        labels[labels < 0] = -1\n",
    "        labels[labels > 0] = 1\n",
    "        optimizer.zero_grad()\n",
    "        #print(points.shape)\n",
    "        outputs = model(points, neural_encoding)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        #optimizer.step()\n",
    "        label_signs = torch.sign(labels)\n",
    "        output_signs = torch.sign(outputs)\n",
    "        correct_signs = (label_signs == output_signs).sum().item()\n",
    "        running_acc += correct_signs / num_points\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / num_epochs, running_acc / num_epochs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(val_loader_1024))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDFRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, hidden_dim):\n",
    "        super(SDFRegressionModel, self).__init__()\n",
    "        self.x_fc = nn.Linear(input_dim, hidden_dim)\n",
    "        self.laten_fc = nn.Linear(latent_dim, hidden_dim) \n",
    "        self.batch_norm1 = nn.BatchNorm1d(2* hidden_dim, affine=True)\n",
    "        self.fc1 = nn.Linear(2* hidden_dim, 2*hidden_dim)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(2*hidden_dim, affine=True)\n",
    "        self.fc2 = nn.Linear(2*hidden_dim, hidden_dim)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_dim, affine=True)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.batch_norm4 = nn.BatchNorm1d(hidden_dim, affine=True)\n",
    "        self.fc4 = nn.Linear(hidden_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x, latent_encoding):\n",
    "        x = self.relu(self.x_fc(x))\n",
    "        latent_encoding = self.relu(self.laten_fc(latent_encoding))\n",
    "        x = torch.cat((x, latent_encoding), dim=-1)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.batch_norm2(self.relu(self.fc1(x)))\n",
    "        x = self.batch_norm3(self.relu(self.fc2(x)))\n",
    "        x = self.batch_norm4(self.relu(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.76623376623377\n",
      "45.422077922077925\n",
      "47.61363636363637\n",
      "48.87337662337662\n",
      "49.87987012987013\n",
      "50.86363636363637\n",
      "51.506493506493506\n",
      "52.51948051948052\n"
     ]
    }
   ],
   "source": [
    "sdf_model = SDFRegressionModel(3, 256, 256)\n",
    "#sdf_model = SDFRegressionModel(3, 256, 64)\n",
    "#sdf_model.load_state_dict(torch.load(\"sdf_model_1024_256.pth\"))\n",
    "sdf_model.to(device)\n",
    "#sdf_model = my_model\n",
    "#sdf_model.eval()\n",
    "optimizer = optim.AdamW(sdf_model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "model.to(device)\n",
    "for i in range(50): \n",
    "    counter = 0\n",
    "    sum_correct_signs = 0\n",
    "    for point_cloud in train_loader_1024:\n",
    "        counter += 1\n",
    "        points, labels, neural_encoding = collect_points_batch(point_cloud, model, num_points=2, batches=64)\n",
    "        perm = torch.randperm(points.shape[0])\n",
    "        points, labels, neural_encoding = points[perm], labels[perm], neural_encoding[perm]\n",
    "        labels[labels < 0] = -1\n",
    "        labels[labels > 0] = 1\n",
    "        optimizer.zero_grad()\n",
    "        outputs = sdf_model(points, neural_encoding)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        label_signs = torch.sign(labels)\n",
    "        output_signs = torch.sign(outputs)\n",
    "        correct_signs = (label_signs == output_signs).sum().item()\n",
    "        sum_correct_signs += correct_signs\n",
    "        #print(correct_signs)\n",
    "        #print(f'{i} Loss: {loss.item():.4f} {len(point_cloud[\"filename\"])}')\n",
    "    print(sum_correct_signs / (counter * 2 * 1))\n",
    "    my_model = sdf_model.to('cpu')\n",
    "    torch.save(my_model.state_dict(), \"sdf_model_1024_256.pth\")    \n",
    "    sdf_model.to(device)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
